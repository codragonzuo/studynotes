
## KVM动态迁移与性能优化
 

一、迁移概述

虚拟机的迁移是指在VMM（Virtual Machine Monitor）上运行的虚拟机系统，能够被转移到其它物理机上的VMM上运行

1.KVM迁移类型
静态迁移

静态迁移也叫做常规迁移、离线迁移（Offline Migration）就是在虚拟机关机的情况下从一台物理机迁移到另一台物理机；因为虚拟机的文件系统是建立在虚拟机镜像上，所以在虚拟机关机的情况下，只需要简单的迁移虚拟机镜像和相应的配置文件到外一台物理机上 如果要保存虚拟机迁移之前的状态，在迁移之前将虚拟机暂停，然后拷贝状态至目的主机，最后在目的主机重建虚拟状态，恢复执行 这种方式的迁移过程需要停止虚拟机的运行，从用户角度看，有明确的一段停机时间，这段时间虚拟机上的服务不可用，但这种迁移方式简单易用，适用于对可用性的要求不高的场合

共享存储的动态迁移



动态迁移（Live Migration）也叫做在线迁移（Online Migration）就在保证虚拟机上服务正常运行的同时，将一个虚拟机系统从一个物理主机移动到另一个物理主机的过程，该过程不会对最终用户造成明显影响，从而使得管理员能够在不影响用户正常使用的情况下，对物理服务器进行离线维修或者升级 与静态迁移不同的是，为了保证迁移过程中虚拟机服务的可用，迁移过程仅有非常短暂的停机时间，迁移的前面阶段，服务在源主机的虚拟机上运行，当迁移进行到一定阶段，目的主机己经具备了运行虚拟机系统的必要资源，经过一个非常短暂的切换，源主机将控制权转移到目的主机，虚拟机系统在目的主机上继续运行 对于虚拟机服务本身而言，由于切换的时间非常短暂，用户感觉不到服务的中断，因而迁移过程对于用户来说是透明的，因此动态迁移适用于对虚拟机服务可用性要求很高的场合 目前主流的动态迁移工具，VMware的VMotion，Citrix的XenMotion，他们都依赖于物理机采用的共享存储，因而在迁移时只需要进行虚拟机系统内存执行状态的迁移，从而获得较好的迁移性能


![](https://linuxli.oss-cn-beijing.aliyuncs.com/KVM/KVM_%E5%8A%A8%E6%80%81%E8%BF%81%E7%A7%BB/1.png)

图上中所示的动态迁移，为了缩短迁移时间，源主机和目的主机采用NFS共享存储；这样，动态迁移只需要考虑虚拟机系统内存执行状态的迁移，从而获得较好的性能

本地存储的动态迁移

动态迁移基于共享存储设备，为的是加速迁移的过程，尽量减少宕机时间，但是在某些情况下需要进行基于本地存储的虚拟机的动态迁移，这就需要存储块动态迁移技术，简称块迁移 比如某些服务器没有使用共享存储，而且迁移的频率很小，虚拟机上的服务对迁移时间的要求不严格，则可以使用存储块动态迁移技术；尽管共享存储能够提高和系统的稳定性，对于中小型企业仅仅为了加快迁移速度而配置昂责的存储设备，性价比不高 在集中式共享外部存储的环境下，基于共享存储的迁移技术在这种场合下会受到限制，虚拟机迁移到目的主机后，不能再访问其原有的外存设备 为拓宽动态迁移技术的应用范围，有必要实现一个包括虚拟机外存迁移在内的全系统动态迁移方案，使得在采用分散式本地存储的计算机集群环境下，仍然能够利用迁移技术转移虚拟机环境，并且保证迁移过程中虚拟机系统服务的可用性

![](https://linuxli.oss-cn-beijing.aliyuncs.com/KVM/KVM_%E5%8A%A8%E6%80%81%E8%BF%81%E7%A7%BB/2.png?x-oss-process=style/123)




上图相比较基于共享存储的动态迁移，数据块动态迁移需要同时迁移虚拟机磁盘镜像和虚拟机系统内存，延长了迁移时间，在迁移性能上会大打折扣

2.虚拟机管理工具
KVM仅仅是Linux内核的一个模块，管理和创建完整的KYM虚拟机，需要更多的辅助工具

QEMU-KVM：QEMU是一个强大的虚拟化软件，它可以虚拟不同的CPU构架，比如说在X86的CPU上虚拟一个Power的CPU，并利用它编译出可运行在Power上的程序；KYM使用了QEMU的基于X86的部分，并稍加改造，形成可控制KVM内核模块的用户空间工具QEMU-KVM，所以Linux发行版中分为kernel部分的KVM内核模块和QEMU-KVM工具；这就是KYM和QEMU的关系

libvirt、virsh、virt-manager：尽管QEMU-KVM工具可以创建和管理KVM虚拟机，但Red Hat依然为KVM开发了更多的辅助工具，比如libvirt、libguestfs等，原因是QEMU工具效率不高，不易于使用；libvirt是一套提供了多种语言接口的API，为各种虚拟化工具提供一套方便、可靠的编程接口，不仅支持KVM，而且支持Xen等其他虚拟机；使用libvirt, 需要通过libvirt提供的函数连接到KVM或Xen宿主机，便可以用同样的命令控制不同的虚拟机；当然libvirt不仅提供了API，还自带一套基于文本的管理虚拟机的命令virsh，可以通过使用virsh命令来使用libvirt的全部功能；但最终用户更渴望的是图形用户界面，这就是virt-manager，他是一套用python编写的虚拟机管理图形界面，用户可以通过它直观地操作不同的虚拟机，virt-manager就是利用Iibvirt的API实现的


##KVM实战：原理、进阶与性能调优》一2.5　与QEMU/KVM结合的组件
 
2.5　与QEMU/KVM结合的组件

在KVM虚拟化的软件栈中，毋庸置疑的是KVM内核模块与QEMU用户态程序是处于最核心的位置，有了它们就可通过qemu命令行操作实现完整的虚拟机功能，本书中多数的实践范例正是通过qemu命令行来演示的。然而，在实际的云计算的虚拟化场景中，为了更高的性能或者管理的方便性，还有很多的软件可以作为KVM虚拟化实施中的组件，这里简单介绍其中的几个。

1. vhost-net

vhost-net是Linux内核中的一个模块，它用于替代QEMU中的virtio-net用户态的virtio网络的后端实现。使用vhost-net时，还支持网卡的多队列，整体来说会让网络性能得到较大提高。在6.1.6节中对vhost-net有更多的介绍。

2. Open vSwitch

Open vSwitch是一个高质量的、多层虚拟交换机，使用开源Apache2.0许可协议，主要用可移植性强的C语言编写的。它的目的是让大规模网络自动化可以通过编程扩展，同时仍然支持标准的管理接口和协议（例如NetFlow、sFlow、SPAN、RSPAN、CLI、LACP、802.1ag）。同时也提供了对 OpenFlow 协议的支持，用户可以使用任何支持 OpenFlow 协议的控制器对 OVS 进行远程管理控制。Open vSwitch被设计为支持跨越多个物理服务器的分布式环境，类似于VMware的vNetwork分布式vswitch或Cisco Nexus 1000 V。Open vSwitch支持多种虚拟化技术，包括Xen/XenServer、KVM和VirtualBox。在KVM虚拟化中，要实现软件定义网络（SDN），那么Open vSwitch是一个非常好的开源选择。

3. DPDK

DPDK全称是Data Plane Development Kit，最初是由Intel公司维护的数据平面开发工具集，为Intel x86处理器架构下用户空间高效的数据包处理提供库函数和驱动的支持，现在也是一个完全独立的开源项目，它还支持POWER和ARM处理器架构。不同于Linux系统以通用性设计为目的，它专注于网络应用中数据包的高性能处理。具体体现在DPDK应用程序是运行在用户空间上，利用自身提供的数据平面库来收发数据包，绕过了Linux内核协议栈对数据包处理过程。其优点是：性能高、用户态开发、出故障后易恢复。在KVM架构中，为了达到非常高的网络处理能力（特别是小包处理能力），可以选择DPDK与QEMU中的vhost-user结合起来使用。

4. SPDK

SPDK全称是Storage Performance Development Kit，它可为编写高性能、可扩展的、用户模式的存储程序提供一系列工具及开发库。它与DPDK非常类似，其主要特点是：将驱动放到用户态从而实现零拷贝、用轮询模式替代传统的中断模式、在所有的I/O链路上实现无锁设计，这些设计会使其性能比较高。在KVM中需要非常高的存储I/O性能时，可以将QEMU与SPDK结合使用。

5. Ceph

Ceph是Linux上一个著名的分布式存储系统，能够在维护 POSIX 兼容性的同时加入复制和容错功能。Ceph由储存管理器（Object storage cluster对象存储集群，即OSD守护进程）、集群监视器（Ceph Monitor）和元数据服务器（Metadata server cluster，MDS）构成。其中，元数据服务器MDS仅仅在客户端通过文件系统方式使用Ceph时才需要。当客户端通过块设备或对象存储使用Ceph时，可以没有MDS。Ceph支持3种调用接口：对象存储，块存储，文件系统挂载。在libvirt和QEMU中都有Ceph的接口，所以Ceph与KVM虚拟化集成是非常容易的。在OpenStack的云平台解决方案中，Ceph是一个非常常用的存储后端。

6. libguestfs

libguestfs是用于访问和修改虚拟机的磁盘镜像的一组工具集合。libguestfs提供了访问和编辑客户机中的文件、脚本化修改客户机中的信息、监控磁盘使用和空闲的统计信息、P2V、V2V、创建客户机、克隆客户机、备份磁盘内容、格式化磁盘、调整磁盘大小等非常丰富的功能。libguestfs还提供了共享库，可以在C/C++、Python等编程语言中对其进行调用。libguestfs不需要启动KVM客户机就可以对磁盘镜像进行管理，功能强大且非常灵活，是管理KVM磁盘镜像的首选工具。


## KVM高级功能

What's QEMU

![](https://upload-images.jianshu.io/upload_images/3143954-5bc6dadc4be158ec.jpg)

QEMU是一个主机上的VMM（virtual machine monitor）,通过动态二进制转换来模拟CPU，并提供一系列的硬件模型，使guest os认为自己和硬件直接打交道，其实是同QEMU模拟出来的硬件打交道，QEMU再将这些指令翻译给真正硬件进行操作。通过这种模式，guest os可以和主机上的硬盘，网卡，CPU，CD-ROM，音频设备和USB设备进行交互。但由于所有指令都需要经过QEMU来翻译，因而性能会比较差：


What's KVM?

KVM实际是linux内核提供的虚拟化架构，可将内核直接充当hypervisor来使用。KVM需要处理器硬件本身支持虚拟化扩展，如intel VT 和AMD AMD-V技术。KVM自2.6.20版本后已合入主干并发行，除此之外，还以模块形式被移植到FreeBSD和illumos中。除了支持x86的处理器，同时也支持S/390,PowerPC,IA-61以及ARM等平台。

![](https://upload-images.jianshu.io/upload_images/3143954-cdde1444c6a86c60.jpg)

工作原理

KVM包含一个内核模块kvm.ko用来实现核心虚拟化功能，以及一个和处理器强相关的模块如kvm-intel.ko或kvm-amd.ko。KVM本身不实现任何模拟，仅仅是暴露了一个/dev/kvm接口，这个接口可被宿主机用来主要负责vCPU的创建，虚拟内存的地址空间分配，vCPU寄存器的读写以及vCPU的运行。有了KVM以后，guest os的CPU指令不用再经过QEMU来转译便可直接运行，大大提高了运行速度。但KVM的kvm.ko本身只提供了CPU和内存的虚拟化，所以它必须结合QEMU才能构成一个完整的虚拟化技术，也就是下面要介绍的技术。

What's QEMU-KVM

从前面的介绍可知，KVM负责cpu虚拟化+内存虚拟化，实现了cpu和内存的虚拟化，但kvm并不能模拟其他设备，还必须有个运行在用户空间的工具才行。KVM的开发者选择了比较成熟的开源虚拟化软件QEMU来作为这个工具，QEMU模拟IO设备（网卡，磁盘等），对其进行了修改，最后形成了QEMU-KVM。


在QEMU-KVM中，KVM运行在内核空间，QEMU运行在用户空间，实际模拟创建、管理各种虚拟硬件，QEMU将KVM整合了进来，通过/ioctl 调用 /dev/kvm，从而将CPU指令的部分交给内核模块来做，KVM实现了CPU和内存的虚拟化，但KVM不能虚拟其他硬件设备，因此qemu还有模拟IO设备（磁盘，网卡，显卡等）的作用，KVM加上QEMU后就是完整意义上的服务器虚拟化。

综上所述，QEMU-KVM具有两大作用：

提供对cpu，内存（KVM负责），IO设备（QEMU负责）的虚拟

对各种虚拟设备的创建，调用进行管理（QEMU负责）

这个方案中，QEMU模拟其他的硬件，如Network, Disk，同样会影响这些设备的性能。于是又产生了pass through半虚拟化设备virtio_blk, virtio_net，提高设备性能。


QEMU-KVM，是QEMU的一个特定于KVM加速模块的分支，里面包含了很多关于KVM的特定代码，与KVM模块一起配合使用。


![](https://upload-images.jianshu.io/upload_images/3143954-ab897227bb8acc26.jpg)

目前QEMU-KVM已经与QEMU合二为一，所有特定于KVM的代码也都合入了QEMU，当需要与KVM模块配合使用的时候，只需要在QEMU命令行加上 --enable-kvm就可以。


## 半虚拟化驱动
virtio 概述

KVM 是必须使用硬件虚拟化辅助技术（如 Intel VT-x 、AMD-V）的 Hypervisor，在CPU 运行效率方面有硬件支持，其效率是比较高的；在有 Intel EPT 特性支持的平台上，内存虚拟化的效率也较高。 QEMU/KVM 提供了全虚拟化环境，可以让客户机不经过任何修改就能运行在 KVM 环境中。不过 KVM 在 I/O虚拟化方面，传统的方式是使用 QEMU 纯软件的方式来模拟 I/O 设备（如网卡、磁盘、显卡等），其效率并不非常高。

### CPU 和 内存的虚拟化由KVM内核模块提供，I/O设备虚拟化由QEMU负责实现。

在KVM中，可以在客户机中使用半虚拟化驱动的方式是采用 Virtio 这个 Linux 上的设备驱动标准框架。

 

完全虚拟化：GuestOS 运行在物理机上的 Hypervisor 之上，GuestOS 并不知道它已经被虚拟化，并不需要任何修改就能工作；

半虚拟化：GuestOS 不仅知道它运行在 Hypervisor 之上，还包括让 GuestOS 更高效的过度到 Hypervisor 的代码。

 

在完全虚拟化模式中，hypervisor 必须模拟设备硬件，它是在会话的最低级别进行模拟的，尽管在该抽象中模拟很干净，但它同时也是最低效的，最复杂的。

在半虚拟化中，GuestOS 和 hypervisor 能够共同合作，让模拟更加高效，缺点是操作系统知道它被虚拟化，并且需要修改才能工作。


![](https://img2018.cnblogs.com/blog/828019/201907/828019-20190708152218290-254841590.png)
 

左图在传统的完全虚拟化环境中,hypervisor必须捕捉这些请求,然后模拟物理硬件的行为。尽管这也做提供很大的灵活性(即运行未更改的操作系统),但它的效率比较低.

右图,半虚拟化,来宾操作系统知道它运行在hypervisor之上,并包含了充当当前的驱动程序.hypervisor为特定的设备模拟实现后端驱动程序.通过在这些前端和后端驱动程序中的virtio，为开发模拟设备提供标准化接口,从而增加代码的跨平台重用率并提高效率.

 

QEMU模拟 I/O 设备基本原理和优缺点


![](https://img2018.cnblogs.com/blog/828019/201907/828019-20190708152521150-1489635258.png)

 

使用QEMU模拟 I/O 的情况下：

　　（1）当客户机中的设备驱动程序（device driver）发起 I/O 操作请求时，KVM模块中的 I/O 操作捕获代码会拦截这次 I/O 请求；

　　（2）经过处理后将本次 I/O 请求的信息存放到 I/O 共享页，并通知用户控件中的 QEMU 程序；

　　（3）QEMU 模拟程序获得 I/O 操作的具体信息之后，交由硬件模拟代码来模拟出本次的 I/O 操作；

　　（4）完成之后，将结果放回到 I/O 共享页，并通知KVM 模块中的 I/O 操作捕获代码；

　　（5）由 KVM 模块中的捕获代码读取 I/O 共享页中的操作结果，并把结果返回到客户机中。

在这个过程中，QEMU进程在等待I/O时被阻塞，当客户机通过 DMA 访问大块 I/O时，QEMU模拟程序将不会把操作结果放到I/O共享页中，而是通过内存映射的方式将结果直接写到客户机的内存中去，然后通过KVM模块告诉客户机DMA操作已经完成。

QEMU 模拟 I/O 设备的方式，其优点是可以通过软件模拟出各种各样的硬件设备，包括一些不常用的或者很老很经典的设备，而且它不用修改客户机操作系统，就可以实现模拟设备在客户机中正常工作。 在KVM客户机中使用这种方式，对于解决手上没有足够设备的软件开发及调试有非常大的好处。而它的缺点是，每次 I/O 操作的路径比较长，有较多的 VMEntry、VMExit发生，需要多次上下文切换（context switch），也需要多次数据复制，所以它的性能较差。

 

半虚拟化 virtio 的基本原理和优缺点

![](https://img2018.cnblogs.com/blog/828019/201907/828019-20190708155532810-988648848.png)



其中前端驱动（frondend，如 virtio-blk、virtio-net等）是在客户机中存在的驱动程序模块，而后端处理程序（backend）是在 QEMU中实现的。

在这前后端驱动之间，还定义了两层来支持客户机与 QEMU 之间的通信：

　　virtio：虚拟队列接口，在概念上将前端驱动程序附加到后端处理程序，一个前端驱动程序可以使用 0 个或 多个队列，具体数据取决于需求；

　　例如：virtio-net 网络驱动程序使用两个虚拟队列（一个用于接收、另一个用于发送），而virtio-blk块驱动程序仅使用一个虚拟队列。虚拟队列实际上被实现为跨越客户机操作系统和hypervisor的衔接点，但它可以通过任意方式实现，前提是客户机操作系统和virtio后端程序都遵循一定的标准，以相互匹配的方式实现它。

　　virtio-ring：实现了环形缓冲区（ring buffer），用于保存前端驱动和后端处理程序执行的信息，并且它们可以一次性保存前端驱动的多次I/O请求，并且交由后端去批量处理，最后实际调用宿主机中设备驱动实现物理上的I/O操作，这样做就可以根据约定实现批量处理而不是客户机中每次I/O请求都需要处理一次，从而提高客户机与hypervisor信息交换的效率。

virtio 半虚拟化驱动的方式，可以获得很好的I/O性能，其性能几乎可以达到和 native（非虚拟化环境中的原生系统）差不多的I/O性能。所以，在使用 kvm 时，如果宿主机内核和客户机都支持 virtio的情况下，一般推荐使用 virtio 达到更好的性能。

 

virtio 缺点：

![](https://img2018.cnblogs.com/blog/828019/201907/828019-20190708160426498-425618492.png)

　　必须要安装特定的virtio驱动使其知道是运行在虚拟化环境中，且按照 virtio 的规定格式进行数据传输，不过客户机中可能有一些老的Linux系统不支持virtio 和主流windows系统需要安装特定的驱动才支持 virtio，不过，较新的一些Linux发行版(如RHEL 6.3、Fedora 17等)默认都将virtio相关驱动编译为模块，可直接作为客户机使用virtio，而且对于主流Windows系统都有对应的virtio驱动程序可供下载使用。

virtio是对半虚拟化hypervisor中的一组通用模拟设备的抽象.该设置还允许hypervisor导出一组通用的模拟设备,并通过一个通用的应用程序接口(API)让它们变得可用.有了半虚拟化hypervisor之后,来宾操作系统能够实现一组通用的接口,在一组后端驱动程序之后采用特定的设备模拟.后端驱动程序不需要是通用的,因为它们只实现前端所需的行为.



注意，在现实中（尽管不需要），设备模拟发生在使用 QEMU 的空间，因此后端驱动程序与 hypervisor 的用户空间交互，以通过 QEMU 为 I/O 提供便利。QEMU 是一个系统模拟器，它不仅提供来宾操作系统虚拟化平台，还提供整个系统（PCI 主机控制器、磁盘、网络、视频硬件、USB 控制器和其他硬件元素）的模拟。   
